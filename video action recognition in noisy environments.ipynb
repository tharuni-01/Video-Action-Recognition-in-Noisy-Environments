{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MAHESH REDDY\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\MAHESH REDDY\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "c:\\Users\\MAHESH REDDY\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "# import remotezip as rz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# Import the MoViNet model from TensorFlow Models (tf-models-official) for the MoViNet model\n",
    "from official.projects.movinet.modeling import movinet\n",
    "from official.projects.movinet.modeling import movinet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_frames(frame, output_size):\n",
    "  \"\"\"\n",
    "    Pad and resize an image from a video.\n",
    "\n",
    "    Args:\n",
    "      frame: Image that needs to resized and padded. \n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      Formatted frame with padding of specified output size.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "    frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "    # print(\"F:\", len(frame))\n",
    "  except:\n",
    "    pass\n",
    "    print(\"F:\", frame)\n",
    "  return frame\n",
    "\n",
    "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n",
    "  \"\"\"\n",
    "    Creates frames from each video file present for each category.\n",
    "\n",
    "    Args:\n",
    "      video_path: File path to the video.\n",
    "      n_frames: Number of frames to be created per video file.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "  \"\"\"\n",
    "\n",
    "  frame_step = 10\n",
    "  # Read each video frame by frame\n",
    "  result = []\n",
    "  src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "  need_length = 1 + (n_frames - 1) * frame_step\n",
    "  # print(video_path, \"VL:\", video_length, need_length, end = \" | \")\n",
    "\n",
    "  if need_length > video_length:\n",
    "    start = 0\n",
    "  else:\n",
    "    max_start = video_length - need_length\n",
    "    start = random.randint(0, max_start + 1)\n",
    "\n",
    "  # src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "  ret, frame = src.read()\n",
    "  result.append(format_frames(frame, output_size))\n",
    "\n",
    "  for _ in range(n_frames - 1):\n",
    "    for _ in range(frame_step):\n",
    "      ret, frame = src.read()\n",
    "      if frame is None:\n",
    "        print(\".\", end=\"\")\n",
    "        continue\n",
    "      \n",
    "    if ret:\n",
    "      frame = format_frames(frame, output_size)\n",
    "      result.append(frame)\n",
    "    else:\n",
    "      result.append(np.zeros_like(result[0]))\n",
    "  src.release()\n",
    "  result = np.array(result)[..., [2, 1, 0]]\n",
    "\n",
    "  # print(\"RL\", len(result))\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FrameGenerator:\n",
    "  def __init__(self, path, n_frames, training = False):\n",
    "    \"\"\" Returns a set of frames with their associated label. \n",
    "\n",
    "      Args:\n",
    "        path: Video file paths.\n",
    "        n_frames: Number of frames. \n",
    "        training: Boolean to determine if training dataset is being created.\n",
    "    \"\"\"\n",
    "    self.path = path\n",
    "    self.n_frames = n_frames\n",
    "    self.training = training\n",
    "    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
    "    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "\n",
    "  def get_files_and_class_names(self):\n",
    "    # video_paths = list(self.path.glob('*/*.avi'))\n",
    "    video_paths = list(self.path.glob('*/*.mp4'))\n",
    "    classes = [p.parent.name for p in video_paths] \n",
    "    return video_paths, classes\n",
    "\n",
    "  def __call__(self):\n",
    "    video_paths, classes = self.get_files_and_class_names()\n",
    "\n",
    "    pairs = list(zip(video_paths, classes))\n",
    "\n",
    "    if self.training:\n",
    "      random.shuffle(pairs)\n",
    "\n",
    "    for path, name in pairs:\n",
    "      video_frames = frames_from_video_file(path, self.n_frames) \n",
    "      if len(video_frames) >= self.n_frames:\n",
    "\n",
    "        label = self.class_ids_for_name[name] # Encode labels\n",
    "        yield video_frames, label\n",
    "      else:\n",
    "        print(\"Skiped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'a0'\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "backbone = movinet.Movinet(model_id=model_id)\n",
    "backbone.trainable = False\n",
    "\n",
    "# Set num_classes=600 to load the pre-trained weights from the original model\n",
    "model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600)\n",
    "model.build([None, None, None, None, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(batch_size, num_frames, resolution, backbone, num_classes):\n",
    "  \"\"\"Builds a classifier on top of a backbone model.\"\"\"\n",
    "  model = movinet_model.MovinetClassifier(\n",
    "      backbone=backbone,\n",
    "      num_classes=num_classes)\n",
    "  model.build([batch_size, num_frames, resolution, resolution, 3])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
    "                    tf.TensorSpec(shape = (), dtype = tf.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 224\n",
    "batch_size = 8\n",
    "num_frames = 8\n",
    "\n",
    "NUM_CLASSES = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_classifier(batch_size, num_frames, resolution, backbone, NUM_CLASSES)\n",
    "model.compile()\n",
    "model.load_weights(\"model_custom_full.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['boxing',\n",
    " 'clapping',\n",
    " 'hitting_bottle',\n",
    " 'hitting_stick',\n",
    " 'jogging_f_b',\n",
    " 'jogging_side',\n",
    " 'kicking',\n",
    " 'running_f_b',\n",
    " 'running_side',\n",
    " 'stabbing',\n",
    " 'walking_f_b',\n",
    " 'walking_side',\n",
    " 'waving_hands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 13s 13s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([3], dtype=int64)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test2_ds = tf.data.Dataset.from_generator(FrameGenerator(pathlib.Path('./Test Actions1'), num_frames),\n",
    "                                         output_signature = output_signature)\n",
    "test2_ds = test2_ds.batch(batch_size)\n",
    "\n",
    "predicted = model.predict(test2_ds)\n",
    "predicted = tf.concat(predicted, axis=0)\n",
    "predicted = tf.argmax(predicted, axis=1)\n",
    "\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hitting_stick'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names[int(predicted[0])]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
